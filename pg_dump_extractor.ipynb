{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb48386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import subprocess\n",
    "import os\n",
    "from datetime import datetime\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "############################\n",
    "# NOTE: You must have a .pgpass entry to programmatically use pg_restore\n",
    "# Run the following:\n",
    "# > touch .pgpass\n",
    "# > nano .pgpass\n",
    "#       in the file, add your connection string:  \"localhost:5432:mydatabase:myuser:mypassword\"\n",
    "# > chmod 600 ~/.pgpass\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e83fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Configure\n",
    "############################\n",
    "\n",
    "# postgres config\n",
    "db_params = {\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"host\": \"localhost\"\n",
    "}\n",
    "\n",
    "# export to subdirectory (in this working directory)\n",
    "subdir = \"Files\"\n",
    "\n",
    "# dump path\n",
    "backup_file_path = \"~/Downloads/usaspending-db_20230708\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a046f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the subdirectory exists, if not, create it\n",
    "if not os.path.exists(subdir):\n",
    "    os.makedirs(subdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4649b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes a query that outputs a query and runs each sequentially\n",
    "# its the dude playing a dude disguised as another dude\n",
    "def nested_query_processor(query):\n",
    "    with psycopg2.connect(**db_params) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            rows = cur.fetchall()\n",
    "            \n",
    "            # Iterate over the rows and execute each command\n",
    "            for row in rows:\n",
    "                alter_table_command = row[0]\n",
    "                cur.execute(alter_table_command)\n",
    "\n",
    "            conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80820991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all tables in the dump (this is used to get chunks later)\n",
    "cmd_list_tables = f\"pg_restore --list {backup_file_path} | grep TABLE\"\n",
    "list_tables = subprocess.check_output(cmd_list_tables, shell=True).decode().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5ce679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore just the schema, including index, sequence & function definitions\n",
    "cmd_restore_schema = f\"pg_restore --no-owner --role=postgres --clean --schema-only -U {db_params['user']} -d {db_params['dbname']} {backup_file_path}\"\n",
    "subprocess.run(cmd_restore_schema, shell=True)\n",
    "\n",
    "# this will output a bunch of warnings that look like errors\n",
    "# if successful, the final output will be something like \"CompletedProcess(...)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b952ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have the full scaffolding, remove any constraints\n",
    "\n",
    "remove_constraints = \"\"\"\n",
    "    select \n",
    "        format('alter table %I.%I drop constraint if exists %I cascade;',table_schema, table_name, constraint_name)\n",
    "    from information_schema.constraint_table_usage\n",
    "    where table_schema not in ('pg_catalog')\n",
    "    ;\"\"\"\n",
    "\n",
    "nested_query_processor(remove_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c1ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove indexes with other index dependencies first\n",
    "# todo: create a topical graph to handle nested dependencies\n",
    "\n",
    "remove_d = \"\"\"\n",
    "    select \n",
    "        distinct \n",
    "        format('drop index if exists %I.%I cascade;', n1.nspname, c1.relname) as com\n",
    "    from\n",
    "        pg_catalog.pg_depend d\n",
    "    join pg_catalog.pg_class c1 on c1.oid = d.refobjid\n",
    "    join pg_catalog.pg_class c2 on c2.oid = d.objid\n",
    "    join pg_catalog.pg_namespace n1 on c1.relnamespace = n1.oid\n",
    "    join pg_catalog.pg_namespace n2 on c2.relnamespace = n2.oid\n",
    "    join pg_catalog.pg_indexes as i on n1.nspname = i.schemaname and c1.relname = i.indexname\n",
    "    where  c2.relkind = 'i' -- this will only include index objects\n",
    "        and c1.relkind not in ('r')\n",
    "        and n1.nspname not in ('pg_catalog','information_schema')\n",
    "        and n1.nspname !~ '^pg_toast'::text\n",
    "    ;\"\"\"\n",
    "\n",
    "nested_query_processor(remove_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfca994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all remaining indexes\n",
    "\n",
    "remove_i = \"\"\"\n",
    "    select\n",
    "        format('drop index if exists %I.%I cascade;', n.nspname, c_ind.relname)\n",
    "    from pg_catalog.pg_index ind\n",
    "    join pg_catalog.pg_class c_ind on c_ind.oid = ind.indexrelid\n",
    "    join pg_catalog.pg_namespace n on n.oid = c_ind.relnamespace\n",
    "    left join pg_catalog.pg_constraint cons on cons.conindid = ind.indexrelid\n",
    "    where 1=1\n",
    "        and n.nspname not in ('pg_catalog','information_schema')\n",
    "        and n.nspname !~ '^pg_toast'::text\n",
    "        and cons.oid is null\n",
    "    ;\"\"\"\n",
    "\n",
    "nested_query_processor(remove_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f035cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main process to restore table, export to csv, gzip, delete csv, truncate table\n",
    "\n",
    "def process_tables(list_tables):\n",
    "    with psycopg2.connect(**db_params) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            for table_line in list_tables:\n",
    "                parts = table_line.split()\n",
    "                if 'TABLE' in parts[3] and 'DATA' in parts[4]:\n",
    "                    table_schema = parts[5]\n",
    "                    table_name = parts[6]\n",
    "                    full_table_name = f\"{table_schema}.{table_name}\"\n",
    "\n",
    "                    print(f\"Restoring Table: {full_table_name} @ \", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                    cmd_restore_table = f\"pg_restore --no-owner --role=postgres -U {db_params['user']} -d {db_params['dbname']} -t {table_name} {backup_file_path}\"\n",
    "                    subprocess.run(cmd_restore_table, shell=True)\n",
    "\n",
    "                    csv_file_path = os.path.join(subdir, f\"{full_table_name.replace('.', '_')}.csv\")\n",
    "                    gz_file_path = os.path.join(subdir, f\"{full_table_name.replace('.', '_')}.csv.gz\")\n",
    "\n",
    "                    #print(f\"Exporting CSV: {full_table_name}\")\n",
    "                    with open(csv_file_path, 'w') as f:\n",
    "                        cur.copy_expert(f\"COPY {full_table_name} TO STDOUT WITH CSV HEADER\", f)\n",
    "\n",
    "                    #print(f\"Gzip: {full_table_name}\")\n",
    "                    cmd_gzip_and_split = f\"gzip -c {csv_file_path} | split -b 1000m - {gz_file_path}_part_\"\n",
    "                    subprocess.run(cmd_gzip_and_split, shell=True)\n",
    "\n",
    "                    #print(f\"Deleted original CSV: {csv_file_path}\")\n",
    "                    os.remove(csv_file_path)\n",
    "\n",
    "                    cur.execute(f\"truncate table {full_table_name}\")\n",
    "                    print(f\"{full_table_name} Completed @ \", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "                    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaef084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chunks of tables for threading\n",
    "tables_at_a_time = 10 # set concurrency\n",
    "chunks = [list_tables[i:i+tables_at_a_time] for i in range(0, len(list_tables), tables_at_a_time)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dbd55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute main process\n",
    "# thread pool to process chunks of tables concurrently\n",
    "with ThreadPoolExecutor(max_workers=tables_at_a_time) as executor:\n",
    "    executor.map(process_tables, chunks)\n",
    "\n",
    "# you'll get a lot of pg_restore warnings; they can be ignored."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
